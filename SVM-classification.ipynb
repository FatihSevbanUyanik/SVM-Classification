{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SVM Classification</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# importing dataset\n",
    "pdTestFeatures  = pd.read_csv( os.path.join(cwd, \"data\", \"test-features.csv\"),  header=None)\n",
    "pdTestLabels    = pd.read_csv( os.path.join(cwd, \"data\", \"test-labels.csv\"),    header=None)\n",
    "pdTrainFeatures = pd.read_csv( os.path.join(cwd, \"data\", \"train-features.csv\"), header=None)\n",
    "pdTrainLabels   = pd.read_csv( os.path.join(cwd, \"data\", \"train-labels.csv\"),   header=None)\n",
    "    \n",
    "# labeling as 0 or 1 the dataset\n",
    "pdTestLabels[ pdTestLabels[0] <  90 ] = 0\n",
    "pdTestLabels[ pdTestLabels[0] >= 90 ] = 1\n",
    "pdTrainLabels[ pdTrainLabels[0] <  90 ] = 0\n",
    "pdTrainLabels[ pdTrainLabels[0] >= 90 ] = 1\n",
    "    \n",
    "# dataset \n",
    "npTestFeatures  = pdTestFeatures.values\n",
    "npTestLabels    = pdTestLabels.values\n",
    "npTrainFeatures = pdTrainFeatures.values\n",
    "npTrainLabels   = pdTrainLabels.values\n",
    "    \n",
    "# properties\n",
    "testSampleCount  = npTestFeatures.shape[0]\n",
    "trainSampleCount = npTrainFeatures.shape[0]\n",
    "featureCount = npTestFeatures.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing test results for SVM  soft margin.\n",
    "def printTestResultsForSoftMargin(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels, optimalC):\n",
    "    from sklearn.svm import SVC\n",
    "    svmSoftMarginModel = SVC(random_state=1, C=optimalC)\n",
    "    svmSoftMarginModel.fit(trainFeatures, trainLabels)\n",
    "    predictions = svmSoftMarginModel.predict(npTestFeatures).reshape(-1,1)\n",
    " \n",
    "    truePositives  = np.sum( np.logical_and( npTestLabels, predictions) )\n",
    "    falsePositives = np.sum( np.logical_and( np.logical_not(npTestLabels), predictions) )\n",
    "    falseNegatives  = np.sum( np.logical_and( npTestLabels, np.logical_not(predictions)) )\n",
    "    trueNegatives = np.sum( np.logical_and( np.logical_not(npTestLabels), np.logical_not(predictions)) )\n",
    "\n",
    "    acurracy = ((truePositives + trueNegatives) / testSampleCount)\n",
    "    precision = truePositives / (truePositives + falsePositives) \n",
    "    recall = truePositives / (truePositives + falseNegatives)\n",
    "    negativePredictiveValue = trueNegatives / (falseNegatives + trueNegatives)\n",
    "    falsePositiveRate = falsePositives / (falsePositives + trueNegatives)\n",
    "    falseDiscoveryRate = falsePositives / (falsePositives + truePositives)\n",
    "    f1Score = (2 * precision * recall) / (precision + recall)\n",
    "    f2Score = (5 * precision * recall) / (4 * precision + recall)\n",
    "    \n",
    "    print(\"C-Value \" + str(optimalC) + \": \")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"True Positives: \"  + str(truePositives))\n",
    "    print(\"False Positives: \" + str(falsePositives))\n",
    "    print(\"False Negatives: \" + str(falseNegatives))\n",
    "    print(\"True Negatives: \" + str(trueNegatives))\n",
    "    print(\"Acurracy: \" + str(acurracy))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Negative Predictive Value: \" + str(negativePredictiveValue))\n",
    "    print(\"False Positive Rate: \" + str(falsePositiveRate))\n",
    "    print(\"False Discovery Rate: \" + str(falseDiscoveryRate))\n",
    "    print(\"F1 Score: \" + str(f1Score))\n",
    "    print(\"F2 Score: \" + str(f2Score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing test results for SVM hard margin.\n",
    "def printTestResultsForHardMargin(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels, optimalGamma):\n",
    "    from sklearn.svm import SVC\n",
    "    svmHardMarginModel = SVC(random_state=1, gamma=optimalGamma)\n",
    "    svmHardMarginModel.fit(trainFeatures, trainLabels)\n",
    "    predictions = svmHardMarginModel.predict(npTestFeatures).reshape(-1,1)\n",
    "    \n",
    "    truePositives  = np.sum( np.logical_and( npTestLabels, predictions) )\n",
    "    falsePositives = np.sum( np.logical_and( np.logical_not(npTestLabels), predictions) )\n",
    "    falseNegatives  = np.sum( np.logical_and( npTestLabels, np.logical_not(predictions)) )\n",
    "    trueNegatives = np.sum( np.logical_and( np.logical_not(npTestLabels), np.logical_not(predictions)) )\n",
    "    \n",
    "    acurracy = ((truePositives + trueNegatives) / testSampleCount)\n",
    "    precision = truePositives / (truePositives + falsePositives) \n",
    "    recall = truePositives / (truePositives + falseNegatives)\n",
    "    negativePredictiveValue = trueNegatives / (falseNegatives + trueNegatives)\n",
    "    falsePositiveRate = falsePositives / (falsePositives + trueNegatives)\n",
    "    falseDiscoveryRate = falsePositives / (falsePositives + truePositives)\n",
    "    f1Score = (2 * precision * recall) / (precision + recall)\n",
    "    f2Score = (5 * precision * recall) / (4 * precision + recall)\n",
    "    \n",
    "    print(\"Gamma-Value \" + str(optimalGamma) + \": \")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"True Positives: \"  + str(truePositives))\n",
    "    print(\"False Positives: \" + str(falsePositives))\n",
    "    print(\"False Negatives: \" + str(falseNegatives))\n",
    "    print(\"True Negatives: \" + str(trueNegatives))\n",
    "    print(\"Acurracy: \" + str(acurracy))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Negative Predictive Value: \" + str(negativePredictiveValue))\n",
    "    print(\"False Positive Rate: \" + str(falsePositiveRate))\n",
    "    print(\"False Discovery Rate: \" + str(falseDiscoveryRate))\n",
    "    print(\"F1 Score: \" + str(f1Score))\n",
    "    print(\"F2 Score: \" + str(f2Score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing results of micro and macro averaging for SVM soft margin.\n",
    "def printMicroMacroResultsSoft(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels):\n",
    "    \n",
    "    cValues = np.array([0.001, 0.01, 0.1, 10, 100])\n",
    "    truePositiveArr  = np.zeros(5)\n",
    "    falsePositiveArr = np.zeros(5)\n",
    "    falseNegativeArr = np.zeros(5)\n",
    "    trueNegativeArr  = np.zeros(5)\n",
    "    \n",
    "    for i in range(cValues.shape[0]):\n",
    "        from sklearn.svm import SVC\n",
    "        svmSoftMarginModel = SVC(random_state=1, C=cValues[i])\n",
    "        svmSoftMarginModel.fit(trainFeatures, trainLabels)\n",
    "        predictions = svmSoftMarginModel.predict(npTestFeatures).reshape(-1,1)\n",
    " \n",
    "        truePositives  = np.sum( np.logical_and( npTestLabels, predictions) )\n",
    "        falsePositives = np.sum( np.logical_and( np.logical_not(npTestLabels), predictions) )\n",
    "        falseNegatives  = np.sum( np.logical_and( npTestLabels, np.logical_not(predictions)) )\n",
    "        trueNegatives = np.sum( np.logical_and( np.logical_not(npTestLabels), np.logical_not(predictions)) )\n",
    "        \n",
    "        # adding one to avoid getting nan values \n",
    "        truePositiveArr[i] = truePositives + 1\n",
    "        falsePositiveArr[i] = falsePositives + 1\n",
    "        falseNegativeArr[i] = falseNegatives + 1\n",
    "        trueNegativeArr[i] = trueNegatives + 1\n",
    "\n",
    "    precision = truePositiveArr / (truePositiveArr + falsePositiveArr)\n",
    "    recall = truePositiveArr / (truePositiveArr + falseNegativeArr)\n",
    "    \n",
    "    # MACRO AVERAGING\n",
    "    macroPrecision = precision.mean() \n",
    "    macroRecall = recall.mean()\n",
    "    macroNegativePredictiveValue = (trueNegativeArr / (falseNegativeArr + trueNegativeArr)).mean()\n",
    "    macroFalsePositiveRate = (falsePositiveArr / (falsePositiveArr + trueNegativeArr)).mean()\n",
    "    macroFalseDiscoveryRate = (falsePositiveArr / (falsePositiveArr + truePositiveArr)).mean()\n",
    "    macroF1Score = ((2 * precision * recall) / (precision + recall)).mean()\n",
    "    macroF2Score = ((5 * precision * recall) / (4 * precision + recall)).mean()\n",
    "        \n",
    "    # MICRO AVERAGING\n",
    "    microPrecision = truePositiveArr.sum() / ((truePositiveArr + falsePositiveArr).sum())\n",
    "    microRecall = truePositiveArr.sum() / ((truePositiveArr + falseNegativeArr).sum())\n",
    "    microNegativePredictiveValue = trueNegatives.sum() / ((falseNegatives + trueNegatives).sum())\n",
    "    microFalsePositiveRate = falsePositives.sum() / ((falsePositives + trueNegatives).sum())\n",
    "    microFalseDiscoveryRate = falsePositives.sum() / ((falsePositives + truePositives).sum())\n",
    "    microF1Score = (2 * precision * recall).sum() / ((precision + recall).sum())\n",
    "    microF2Score = (5 * precision * recall).sum() / ((4 * precision + recall).sum())\n",
    "    \n",
    "    # printing results\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-------------------------MACRO AVERAGING-------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"Macro Precision: \" + str(macroPrecision))\n",
    "    print(\"Macro Recall: \" + str(macroRecall))\n",
    "    print(\"Macro Negative Predictive Value: \" + str(macroNegativePredictiveValue))\n",
    "    print(\"Macro False Positive Rate: \" + str(macroFalsePositiveRate))\n",
    "    print(\"Macro False Discovery Rate: \" + str(macroFalseDiscoveryRate))\n",
    "    print(\"Macro F1 Score: \" + str(macroF1Score))\n",
    "    print(\"Macro F2 Score: \" + str(macroF2Score))\n",
    "    print()\n",
    "    \n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-------------------------MICRO AVERAGING-------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"Micro Precision: \" + str(microPrecision))\n",
    "    print(\"Micro Recall: \" + str(microRecall))\n",
    "    print(\"Micro Negative Predictive Value: \" + str(microNegativePredictiveValue))\n",
    "    print(\"Micro False Positive Rate: \" + str(microFalsePositiveRate))\n",
    "    print(\"Micro False Discovery Rate: \" + str(microFalseDiscoveryRate))\n",
    "    print(\"Micro F1 Score: \" + str(microF1Score))\n",
    "    print(\"Micro F2 Score: \" + str(microF2Score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing results of micro and macro averaging for SVM hard margin.\n",
    "def printMicroMacroResultsHard(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels):\n",
    "    \n",
    "    gammaValues = np.array([\n",
    "        math.pow(2, -4), \n",
    "        math.pow(2, -3), \n",
    "        math.pow(2, -2), \n",
    "        math.pow(2,  0), \n",
    "        math.pow(2,  1)\n",
    "    ])\n",
    "    \n",
    "    truePositiveArr  = np.zeros(5)\n",
    "    falsePositiveArr = np.zeros(5)\n",
    "    falseNegativeArr = np.zeros(5)\n",
    "    trueNegativeArr  = np.zeros(5)\n",
    "    \n",
    "    for i in range(gammaValues.shape[0]):\n",
    "        from sklearn.svm import SVC\n",
    "        svmSoftMarginModel = SVC(random_state=1, gamma=gammaValues[i])\n",
    "        svmSoftMarginModel.fit(trainFeatures, trainLabels)\n",
    "        predictions = svmSoftMarginModel.predict(npTestFeatures).reshape(-1,1)\n",
    " \n",
    "        truePositives  = np.sum( np.logical_and( npTestLabels, predictions) )\n",
    "        falsePositives = np.sum( np.logical_and( np.logical_not(npTestLabels), predictions) )\n",
    "        falseNegatives  = np.sum( np.logical_and( npTestLabels, np.logical_not(predictions)) )\n",
    "        trueNegatives = np.sum( np.logical_and( np.logical_not(npTestLabels), np.logical_not(predictions)) )\n",
    "        \n",
    "        # adding one to avoid getting nan values \n",
    "        truePositiveArr[i] = truePositives + 1\n",
    "        falsePositiveArr[i] = falsePositives + 1\n",
    "        falseNegativeArr[i] = falseNegatives + 1\n",
    "        trueNegativeArr[i] = trueNegatives + 1\n",
    "        \n",
    "    precision = truePositiveArr / (truePositiveArr + falsePositiveArr)\n",
    "    recall = truePositiveArr / (truePositiveArr + falseNegativeArr)\n",
    "    \n",
    "    # MACRO AVERAGING\n",
    "    macroPrecision = precision.mean() \n",
    "    macroRecall = recall.mean()\n",
    "    macroNegativePredictiveValue = (trueNegativeArr / (falseNegativeArr + trueNegativeArr)).mean()\n",
    "    macroFalsePositiveRate = (falsePositiveArr / (falsePositiveArr + trueNegativeArr)).mean()\n",
    "    macroFalseDiscoveryRate = (falsePositiveArr / (falsePositiveArr + truePositiveArr)).mean()\n",
    "    macroF1Score = ((2 * precision * recall) / (precision + recall)).mean()\n",
    "    macroF2Score = ((5 * precision * recall) / (4 * precision + recall)).mean()\n",
    "        \n",
    "    # MICRO AVERAGING\n",
    "    microPrecision = truePositiveArr.sum() / ((truePositiveArr + falsePositiveArr).sum())\n",
    "    microRecall = truePositiveArr.sum() / ((truePositiveArr + falseNegativeArr).sum())\n",
    "    microNegativePredictiveValue = trueNegatives.sum() / ((falseNegatives + trueNegatives).sum())\n",
    "    microFalsePositiveRate = falsePositives.sum() / ((falsePositives + trueNegatives).sum())\n",
    "    microFalseDiscoveryRate = falsePositives.sum() / ((falsePositives + truePositives).sum())\n",
    "    microF1Score = (2 * precision * recall).sum() / ((precision + recall).sum())\n",
    "    microF2Score = (5 * precision * recall).sum() / ((4 * precision + recall).sum())\n",
    "    \n",
    "    # printing results\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-------------------------MACRO AVERAGING-------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"Macro Precision: \" + str(macroPrecision))\n",
    "    print(\"Macro Recall: \" + str(macroRecall))\n",
    "    print(\"Macro Negative Predictive Value: \" + str(macroNegativePredictiveValue))\n",
    "    print(\"Macro False Positive Rate: \" + str(macroFalsePositiveRate))\n",
    "    print(\"Macro False Discovery Rate: \" + str(macroFalseDiscoveryRate))\n",
    "    print(\"Macro F1 Score: \" + str(macroF1Score))\n",
    "    print(\"Macro F2 Score: \" + str(macroF2Score))\n",
    "    print()\n",
    "    \n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-------------------------MICRO AVERAGING-------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"Micro Precision: \" + str(microPrecision))\n",
    "    print(\"Micro Recall: \" + str(microRecall))\n",
    "    print(\"Micro Negative Predictive Value: \" + str(microNegativePredictiveValue))\n",
    "    print(\"Micro False Positive Rate: \" + str(microFalsePositiveRate))\n",
    "    print(\"Micro False Discovery Rate: \" + str(microFalseDiscoveryRate))\n",
    "    print(\"Micro F1 Score: \" + str(microF1Score))\n",
    "    print(\"Micro F2 Score: \" + str(microF2Score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Question 4.1: </strong> In this part, you will train a linear SVM model with soft margin without using any kernels. Your model's hyper-parameter is C. Using 10-fold cross validation on your training set, and the optimum C value of your model. Look for the best C value with line search in the following range [10􀀀3; 10􀀀210􀀀1; 10101; 102] and calculate accuracy on the left-out fold. For each value of C, calculate mean cross validation accuracy by changing the left-out fold each time and plot it in a nice form. Report your optimum C value. Then, run your model on the test set with this C value and report test set accuracy with the confusion matrix. Calculate and report micro and macro averages of precision, recall, negative predictive value (NPV), false positive rate (FPR), false discovery rate (FDR), F1 and F2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFT MARGIN\n",
    "splitsFeatures = np.split( npTrainFeatures, 10 )\n",
    "splitsLabels = np.split( npTrainLabels, 10 )\n",
    "cValues = np.array([0.001, 0.01, 0.1, 10, 100])\n",
    "acurraciesSoft = np.zeros((5, 10))\n",
    "\n",
    "for i in range(cValues.shape[0]):\n",
    "    for j in range(10):\n",
    "        trainFeatures = np.zeros((1,8))\n",
    "        trainLabels = np.zeros((1,1))\n",
    "        validationFeatures = np.zeros((1,8))\n",
    "        validationLabels = np.zeros((1,1))\n",
    "    \n",
    "        for k in range(10):\n",
    "            if k != j:\n",
    "                trainFeatures = np.vstack((trainFeatures, splitsFeatures[k]))\n",
    "                trainLabels = np.vstack((trainLabels, splitsLabels[k]))\n",
    "            else:\n",
    "                validationFeatures = np.vstack((validationFeatures, splitsFeatures[k]))\n",
    "                validationLabels = np.vstack((validationLabels, splitsLabels[k]))\n",
    "    \n",
    "        validationFeatures = np.delete(validationFeatures, (0), axis=0)\n",
    "        validationLabels = np.delete(validationLabels, (0), axis=0)\n",
    "        trainFeatures = np.delete(trainFeatures, (0), axis=0)\n",
    "        trainLabels = np.delete(trainLabels, (0), axis=0)\n",
    "    \n",
    "        from sklearn.svm import SVC\n",
    "        svmSoftMargin = SVC(random_state=1, C=cValues[i])\n",
    "        svmSoftMargin.fit(trainFeatures, trainLabels)\n",
    "        acc = svmSoftMargin.score(validationFeatures, validationLabels)\n",
    "        acurraciesSoft[i, j] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracies for Soft Margin\n",
      "--------------------------\n",
      "[0.60371429 0.8455     0.85307143 0.85814286 0.85914286]\n"
     ]
    }
   ],
   "source": [
    "accurraciesAveragesSoft = acurraciesSoft.mean(axis=1)\n",
    "print(\"Acurracies for Soft Margin\")\n",
    "print(\"--------------------------\")\n",
    "print(accurraciesAveragesSoft)\n",
    "# for C = 100, we obtained the best acurrracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-Value 100: \n",
      "-----------------------------------------------------\n",
      "True Positives: 2099\n",
      "False Positives: 68\n",
      "False Negatives: 243\n",
      "True Negatives: 968\n",
      "Acurracy: 0.9079336885731202\n",
      "Precision: 0.9686202122750346\n",
      "Recall: 0.8962425277540563\n",
      "Negative Predictive Value: 0.7993393889347646\n",
      "False Positive Rate: 0.06563706563706563\n",
      "False Discovery Rate: 0.03137978772496539\n",
      "F1 Score: 0.9310268352184519\n",
      "F2 Score: 0.9098396185522324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the test results of C value with highest acurracy (C = 100)\n",
    "printTestResultsForSoftMargin(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "-------------------------MACRO AVERAGING-------------------------\n",
      "-----------------------------------------------------------------\n",
      "Macro Precision: 0.8969372261776188\n",
      "Macro Recall: 0.9318259385665529\n",
      "Macro Negative Predictive Value: 0.7584702454509007\n",
      "Macro False Positive Rate: 0.2926782273603083\n",
      "Macro False Discovery Rate: 0.10306277382238113\n",
      "Macro F1 Score: 0.9082555621715149\n",
      "Macro F2 Score: 0.9205012906528423\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-------------------------MICRO AVERAGING-------------------------\n",
      "-----------------------------------------------------------------\n",
      "Micro Precision: 0.8778938906752412\n",
      "Micro Recall: 0.9318259385665529\n",
      "Micro Negative Predictive Value: 0.7993393889347646\n",
      "Micro False Positive Rate: 0.06563706563706563\n",
      "Micro False Discovery Rate: 0.03137978772496539\n",
      "Micro F1 Score: 0.9099445584925333\n",
      "Micro F2 Score: 0.9204809900459388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing micro and macro average of Soft Margin SVM\n",
    "printMicroMacroResultsSoft(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Question 4.2: </strong> This time, use radial basis function (RBF) kernel to train your hard margin SVM model on the processed (discretized) dataset from Question 3. RBF kernel is de\f",
    "ned a sIn RBF kernel formula, = 􀀀 1 2\u001b2 is a free parameter that can be \f",
    "ne-tuned. This parameter is the inverse of the radius the in uence of samples selected by the model as support vectors. Similar to linear SVM part, train a SVM classi\f",
    "er with RBF kernel using same training and test sets you have used in linear SVM model above. In addition to the penalty parameter C, is your new hyper-parameter that needs be optimized. Using 10-fold cross validation and calculating mean cross validation accuracy as described in Question 4.1, \f",
    "nd and report the best within the interval from the logarithmic scale [2􀀀4; 2􀀀3; 2􀀀220; 21]. After tuning on your training set, run your model on the test set and report your accuracy along with the confusion matrix. Calculate and report micro and macro averages of precision, recall, negative predictive value (NPV), false positive rate (FPR), false discovery rate (FDR), F1 and F2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAMMA VALUES\n",
    "import math\n",
    "splitsFeatures = np.split( npTrainFeatures, 10 )\n",
    "splitsLabels = np.split( npTrainLabels, 10 )\n",
    "gammaValues = np.array([\n",
    "    math.pow(2, -4), \n",
    "    math.pow(2, -3), \n",
    "    math.pow(2, -2), \n",
    "    math.pow(2,  0), \n",
    "    math.pow(2,  1)\n",
    "])\n",
    "\n",
    "acurraciesHard = np.zeros((5, 10))\n",
    "\n",
    "for i in range(gammaValues.shape[0]):\n",
    "    for j in range(10):\n",
    "        trainFeatures = np.zeros((1,8))\n",
    "        trainLabels = np.zeros((1,1))\n",
    "        validationFeatures = np.zeros((1,8))\n",
    "        validationLabels = np.zeros((1,1))\n",
    "    \n",
    "        for k in range(10):\n",
    "            if k != j:\n",
    "                trainFeatures = np.vstack((trainFeatures, splitsFeatures[k]))\n",
    "                trainLabels = np.vstack((trainLabels, splitsLabels[k]))\n",
    "            else:\n",
    "                validationFeatures = np.vstack((validationFeatures, splitsFeatures[k]))\n",
    "                validationLabels = np.vstack((validationLabels, splitsLabels[k]))\n",
    "    \n",
    "        validationFeatures = np.delete(validationFeatures, (0), axis=0)\n",
    "        validationLabels = np.delete(validationLabels, (0), axis=0)\n",
    "        trainFeatures = np.delete(trainFeatures, (0), axis=0)\n",
    "        trainLabels = np.delete(trainLabels, (0), axis=0)\n",
    "    \n",
    "        from sklearn.svm import SVC\n",
    "        svmSoftMargin = SVC(random_state=1, gamma=gammaValues[i])\n",
    "        svmSoftMargin.fit(trainFeatures, trainLabels)\n",
    "        acc = svmSoftMargin.score(validationFeatures, validationLabels)\n",
    "        acurraciesHard[i, j] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracies for Hard Margin\n",
      "--------------------------\n",
      "[0.85864286 0.8565     0.85307143 0.8415     0.8325    ]\n"
     ]
    }
   ],
   "source": [
    "accurraciesAveragesHard = acurraciesHard.mean(axis=1)\n",
    "print(\"Acurracies for Hard Margin\")\n",
    "print(\"--------------------------\")\n",
    "print(accurraciesAveragesHard)\n",
    "# for gamma = 2^-5, we obtained the best acurrracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma-Value 0.0625: \n",
      "-----------------------------------------------------\n",
      "True Positives: 2127\n",
      "False Positives: 79\n",
      "False Negatives: 215\n",
      "True Negatives: 957\n",
      "Acurracy: 0.9129662522202486\n",
      "Precision: 0.9641885766092475\n",
      "Recall: 0.908198121263877\n",
      "Negative Predictive Value: 0.8165529010238908\n",
      "False Positive Rate: 0.07625482625482626\n",
      "False Discovery Rate: 0.035811423390752495\n",
      "F1 Score: 0.9353562005277045\n",
      "F2 Score: 0.9188698807672369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the test results of gamma value with highest acurracy (gamma = 100)\n",
    "printTestResultsForHardMargin(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels, gammaValues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "-------------------------MACRO AVERAGING-------------------------\n",
      "-----------------------------------------------------------------\n",
      "Macro Precision: 0.9616350966102394\n",
      "Macro Recall: 0.9088737201365188\n",
      "Macro Negative Predictive Value: 0.8169058999375503\n",
      "Macro False Positive Rate: 0.08188824662813102\n",
      "Macro False Discovery Rate: 0.038364903389760716\n",
      "Macro F1 Score: 0.9345092501269663\n",
      "Macro F2 Score: 0.9189570526732729\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-------------------------MICRO AVERAGING-------------------------\n",
      "-----------------------------------------------------------------\n",
      "Micro Precision: 0.9616322108874243\n",
      "Micro Recall: 0.9088737201365188\n",
      "Micro Negative Predictive Value: 0.8169257340241797\n",
      "Micro False Positive Rate: 0.08687258687258688\n",
      "Micro False Discovery Rate: 0.04054054054054054\n",
      "Micro F1 Score: 0.9345103005107307\n",
      "Micro F2 Score: 0.9189576960438709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing micro and macro average of Hard Margin SVM\n",
    "printMicroMacroResultsHard(npTrainFeatures, npTrainLabels, npTestFeatures, npTestLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
